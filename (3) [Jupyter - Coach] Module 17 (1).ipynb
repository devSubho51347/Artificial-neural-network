{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you ever wondered how we process and learn information? For example, how does our body process information such that we are able to move our hands or legs? To put it simply, the brain will process information and then send out signals to the rest of the body to trigger certain muscle movements. These signals are transported through the nervous system. One of the main components of the nervous system are neuron cells. These cells work on a threshold basis which means that the signal will only be transferred from cells to cells if it is higher than a certain value or amount. As such, when we decide to move our hands, the signals from the brain will get transferred to the muscle in our hands and not the muscle in our legs.\n",
    "\n",
    "### Training Artificial Neural Networks\n",
    "However, this only explains how we process information. How about the ability of humans to learn? For example, why do we know to stop at a red light or how to kick a ball? It is because we were trained to do so by looking at examples or how other people were doing it. Through these examples, we were able to learn and remember.\n",
    "\n",
    "Would it be great if computers were able to mimic the way humans process and learn information? With artificial neural network, this can be done! Artificial neural networks are able to process and 'learn' complex relationships within datasets. An illustration of a simple neural network is shown below.\n",
    "\n",
    "The basic idea is that we input data into the input layer. Data will be processed in the subsequent hidden layer(s) - we only show one hidden layer on the picture below, but it can be many layers. Each layer is made of multiple artificial neurons which apply functions to the data and pass it on to another hidden layer, finally ending with the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/ANN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The illustration above shows a simple neural network with 1 input layer, 1 hidden layer (in between an input layer and an output layer) and 1 output layer. Each circle represent 1 node or 1 neuron. We usually do not discuss the number of nodes at the input layer within the model architecture as the input layer is just the data that is being passed to the model. Thus, the hidden layer has 4 nodes/neurons and the output layer has 1 node/neuron. \n",
    "\n",
    "The output layer will show the results of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the neural network work? How can they be useful for machine learning project? Watch this [video](https://www.youtube.com/watch?v=aircAruvnKk) to find out more about artificial neural networks. Pause the video and take time to try and understand how the neural network works. Note down any interesting information on neural networks on your worksheet. Are you also able to draw out the network (similar to the illustration above) if the network has 1 input layer with 5 nodes, 2 hidden layers with 3 nodes each and 1 output layer with 2 nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get youths to draw out the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding the different features of an artificial neural network, one question that still remains is how does the network \"learn\"?\n",
    "\n",
    "Take for example a young basketball player who is learning to shoot a 3-point shot. If he shoots and he misses because the shot is too short, the basketball player will adjust and increase the strength of the next shot. If the next shot now is too far right of the basket, the player will again adjust his shot to shoot more towards the center. The player continues to do this until the shot is made. The player then remembers the exact strength and shot direction when shooting 3-point shots in future.\n",
    "\n",
    "This is similar to how neural networks are trained. First, the data is passed through the network and a predicted output is given. This is known as a forward propagation. The predicted output is then compared to the actual output of the data and the differences between the predicted and actual will be passed backwards through the model. During the backward pass, adjustments will be made within the model such that the differences between prediction output and actual output will be reduced. This is known as the backpropagation. After the adjustments are made, data will be passed through from the input layer again and another predicted output will be made. The new predicted output will be compared to the actual output again and the differences will be passed backwards through the model. More adjustments will be made within the model.\n",
    "\n",
    "The process of forward propagation and backpropagation will be repeated until the differences between predicted output and the actual output are minimised. The model is now trained and can be used for prediction of other similar datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of another example in your life that is similar to the way the neural networks are trained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pets to do tricks\n",
    "# Examinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example to help you understand neural networks better. You have been tasked to bake a cake for your father's birthday. You follow the basic cookbook and come up with a basic cake. As you want your cake to be the best cake anyone has eaten, you approach your mother for a taste test. Your mother replies that the cake is too sweet and slightly burnt. Thus, you try and adjust the amount of sugar and also the time the cake spent in the oven. Next, you will then bake another cake with your new recipe and baking time before doing another taste test with your mother. This continues until you get the perfect cake.\n",
    "\n",
    "The first steps to bake the cake are similar to the forward propagation step within the artificial neural network. The taste test is similar to the comparison of a predicted output to the actual output. The adjustment of the amount of sugar and baking time is similar to backpropagation within the neural network. The repetition in the steps are similar to the full training process of the model.\n",
    "\n",
    "To have a better visualisation of how backpropagation works, watch this [video](https://www.youtube.com/watch?v=Ilg3gGewQ5U) and take down any information that interests you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bonus: You are not required to understand the actual adjustments within the model. However, if you are mathematically inclined or really interested in understanding all the adjustments and have time, you can watch the 2 videos listed below. Note down any interesting information in your worksheet or the cell below. </font>\n",
    "- [video 1](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [video 2](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network with the Iris Flower dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the neural network using the Iris Flower dataset!\n",
    "\n",
    "## 1. Import required Libraries\n",
    "Firstly, we will import the required libraries: pandas and numpy to provide data structure, and scikit learn to gain access to artificial neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./resources/PetalSepal1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain and explore dataset\n",
    "\n",
    "Import the Iris Flower dataset as a dataframe df below. \n",
    "The file should be iris.data. \n",
    "\n",
    "Remember to include the headers for all the columns. \n",
    "\n",
    "Refer to the picture (source: https://www.researchgate.net/figure/Trollius-ranunculoide-flower-with-measured-traits_fig6_272514310) above to understand the variables. \n",
    "\n",
    "We'll first explore the data. Do you remember how this is done?\n",
    "1. Open the csv file and put it into a dataframe.\n",
    "2. Include headers\n",
    "3. Use .info() and .describe() to see basic information about the dataset\n",
    "\n",
    "Check for any missing values or erroneous data. Are there any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/iris.data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\", \"class\"]\n",
    "df.columns = names\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determine features and target values\n",
    "\n",
    "Now, we have to split the datset into the x values (features which the model can learn relationships from) and the y values (target values or expected output from the model). \n",
    "\n",
    "### Standardization\n",
    "We would also have to standardize the dataset. \n",
    "What is standardization for? To understand this, look at the distribution of data above! Notice the different mean and standard deviation. Comparing between these variables will be difficult. Standardization helps us to equalize these various distributions into a common mean and standart deviation, so that we can compare them easily. Refer to the [standardscaler graph here](http://benalexkeen.com/feature-scaling-with-scikit-learn/). See how data changed before and after scaling. \n",
    "\n",
    "\n",
    "This is to allow neural networks to classify easily. The code below will extract out the x values as x_values and also standardise the values. We will extract the y_values later. Run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.032057</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.337848</td>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.263460</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0 -0.900681  1.032057 -1.341272 -1.312977\n",
       "1 -1.143017 -0.124958 -1.341272 -1.312977\n",
       "2 -1.385353  0.337848 -1.398138 -1.312977\n",
       "3 -1.506521  0.106445 -1.284407 -1.312977\n",
       "4 -1.021849  1.263460 -1.341272 -1.312977"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "print(x_values.head())\n",
    "standardise = StandardScaler() # the standardscaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1\n",
    "x_values = standardise.fit_transform(x_values)\n",
    "x_values_df = pd.DataFrame(x_values)\n",
    "x_values_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this standardized dataset to the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0 -0.900681  1.032057 -1.341272 -1.312977\n",
      "1 -1.143017 -0.124958 -1.341272 -1.312977\n",
      "2 -1.385353  0.337848 -1.398138 -1.312977\n",
      "3 -1.506521  0.106445 -1.284407 -1.312977\n",
      "4 -1.021849  1.263460 -1.341272 -1.312977\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the .describe function to find out what is the current mean and std value of the standardized dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1             2             3\n",
      "count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02\n",
      "mean  -2.775558e-16 -5.140333e-16  1.154632e-16  9.251859e-16\n",
      "std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00\n",
      "min   -1.870024e+00 -2.438987e+00 -1.568735e+00 -1.444450e+00\n",
      "25%   -9.006812e-01 -5.877635e-01 -1.227541e+00 -1.181504e+00\n",
      "50%   -5.250608e-02 -1.249576e-01  3.362659e-01  1.332259e-01\n",
      "75%    6.745011e-01  5.692513e-01  7.627586e-01  7.905908e-01\n",
      "max    2.492019e+00  3.114684e+00  1.786341e+00  1.710902e+00\n"
     ]
    }
   ],
   "source": [
    "print(x_values_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the neural network\n",
    "\n",
    "We can now build a simple neural network. In order to do so, we will need to import the dense from TensorFlow Keras library and sequential functions from BigDL. \n",
    "\n",
    "### Sequential\n",
    "The Sequential model allows you to first create an empty model object, and then add layers to it one after another in sequence.\n",
    "\n",
    "### Dense\n",
    "A dense layer is simply a layer of neurons in the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: setuptools in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\softwares\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\softwares\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\softwares\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\softwares\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\softwares\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\softwares\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\softwares\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\softwares\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\softwares\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\softwares\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\softwares\\anaconda\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=900549523996283af0804a761e2c4bfdf606e76ff6af51014ec7265cc3981ef4\n",
      "  Stored in directory: c:\\users\\kargw\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-preprocessing-1.1.2 libclang-14.0.1 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a neural network with 1 input layer, 2 hidden layers and 1 output layer. There are no rules to decide how many nodes should be within the hidden layers. \n",
    "\n",
    "For this neural network, we will use 6 hidden nodes for each hidden layer. \n",
    "\n",
    "With regards to the output layer, we should use as many nodes as the number of classes. How many nodes should we use for the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should use 3 nodes in the output layer as there are 3 different flower types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you draw the neural network you are creating? Draw them now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the youths to draw the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the code below to build the artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the neural network as model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. \n",
    "# Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model.add(Dense(6,activation='relu'))\n",
    "\n",
    "model.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. \n",
    "# The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: [Comparisons between activation functions](http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/)\n",
    "You've learnt about ReLu earlier in Acquire-CV. There are many more activation functions. They are like [on-off buttons](https://en.wikipedia.org/wiki/Activation_function) to allow certain data/input to follow through the neurons or not. You are not expected to know the functions in detail for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out the model summary after it has been compiled. Try the code below to see:\n",
    "- The layers and their order in the model.\n",
    "- The output shape of each layer.\n",
    "- The number of parameters (weights) in each layer.\n",
    "- The total number of parameters (weights) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135\n",
      "Trainable params: 135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the y_values are categorical in nature (names of flowers instead of numbers), we have to convert the y_values from categories into numbers before we can train the neural network. For neural networks, if the categories are not numerical groups (for example, 1,2,3,4,etc) we have to perform label encoding (Remember doing this in an earlier notebook?) before doing one-hot encoding. \n",
    "\n",
    "Refer to the bonus section in the \"Supervised learning technique\" notebook for more information on one-hot encoding. You can also read this [article](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) to find out more. We can use the to_categorical function from Keras to help us do so. Try the code below to label encode then one-hot encode the y_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: class, dtype: int64\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: class, dtype: int64\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Print number of data points in each class\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Dictionary to input the different numbers for different classes. \n",
    "# We are using values starting from 0 so that only 3 instead of 4 columns will be created in by the one-hot encoding.\n",
    "label_encode = {\"class\": {\"Iris-setosa\":0, \"Iris-versicolor\":1, \"Iris-virginica\":2}}\n",
    "\n",
    "# Use .replace to change the different classes into numbers\n",
    "df.replace(label_encode,inplace=True)\n",
    "\n",
    "# Print number of data points in each class to check if the classes have changed to numbers\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Extract out the class as y_values\n",
    "y_values = df['class']\n",
    "\n",
    "# One-hot encode the y_values\n",
    "y_values = to_categorical(y_values)\n",
    "\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what is listed in the y_values above. We have encoded the flower names into numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model. Run the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 1s 954us/step - loss: 1.1468 - accuracy: 0.4067\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0932 - accuracy: 0.4467\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 0s 956us/step - loss: 1.0586 - accuracy: 0.4333\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0070 - accuracy: 0.5733\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 0s 995us/step - loss: 0.9419 - accuracy: 0.6467\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8656 - accuracy: 0.6867\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7777 - accuracy: 0.7733\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.6888 - accuracy: 0.8067\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.5962 - accuracy: 0.8133\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.4921 - accuracy: 0.8533\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.3892 - accuracy: 0.8667\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 0s 955us/step - loss: 0.3184 - accuracy: 0.8733\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 0s 948us/step - loss: 0.2752 - accuracy: 0.8867\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 0s 945us/step - loss: 0.2470 - accuracy: 0.9000\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 0s 959us/step - loss: 0.2201 - accuracy: 0.8867\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9267\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9267\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9400\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9400\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9600\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.1227 - accuracy: 0.9600\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1140 - accuracy: 0.9667\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9600\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9600\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9667\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9733\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9600\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9600\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9667\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0788 - accuracy: 0.9600\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 0s 978us/step - loss: 0.0819 - accuracy: 0.9600\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 0s 974us/step - loss: 0.0773 - accuracy: 0.9600\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 0s 963us/step - loss: 0.0793 - accuracy: 0.9733\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.0777 - accuracy: 0.9667\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0682 - accuracy: 0.9733\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9600\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9667\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9533\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9600\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9600\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9733\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9667\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 0s 968us/step - loss: 0.0671 - accuracy: 0.9733\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9667\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9600\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9733\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9600\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.0616 - accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0600 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d3bb50130>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model.fit(x_values,y_values,epochs=50,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have trained your first neural network. Before we look at the accuracy, we have to understand some of the terms in the output.\n",
    "\n",
    "Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "\n",
    "us/step shows how long the model took to train on each epoch.\n",
    "\n",
    "acc shows how accurate the model is. \n",
    "\n",
    "Notice how these numbers change over the different epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model above, what was the accuracy value you've got?\n",
    "\n",
    "Try to see if you can get a better accuracy by adding another hidden layer to the model. The additional hidden layer can have the same number of nodes as the previous layer. \n",
    "\n",
    "Copy the relevant codes listed above in the cell below and modify the codes to add in the additional hidden layer. Train the new model and see if the accuracy improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135\n",
      "Trainable params: 135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model2.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model2.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model2.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model2.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 1.0056 - accuracy: 0.4800\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.7759 - accuracy: 0.6467\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5975 - accuracy: 0.6600\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.6667\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 965us/step - loss: 0.4240 - accuracy: 0.8200\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 941us/step - loss: 0.3795 - accuracy: 0.8400\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8733\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 952us/step - loss: 0.2901 - accuracy: 0.9067\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 989us/step - loss: 0.2495 - accuracy: 0.9400\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9533\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 973us/step - loss: 0.1933 - accuracy: 0.9400\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 910us/step - loss: 0.1706 - accuracy: 0.9400\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9533\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.1414 - accuracy: 0.9600\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9600\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9667\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9600\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0967 - accuracy: 0.9800\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d3cc59370>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model2.fit(x_values,y_values,epochs=20,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding another hidden layer, you should observe that the accuracy seems to be lower than the initial model. As such, this shows that adding more layers need not necessarily lead to a higher accuracy.\n",
    "\n",
    "<font color = blue>Bonus: You can try other methods to improve the accuracy. How about increasing the number of nodes in the hidden layer? Do you get a higher accuracy if you do so?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 10)                50        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 nodes. Input_dim refers to the number of columns/number of features in x_values or input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model3.add(Dense(10,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 10 nodes. \n",
    "model3.add(Dense(10,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model3.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 1s 885us/step - loss: 1.1271 - accuracy: 0.3800\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.7632 - accuracy: 0.6733\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 999us/step - loss: 0.5547 - accuracy: 0.7600\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 883us/step - loss: 0.4519 - accuracy: 0.7933\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8467\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8867\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.2792 - accuracy: 0.9267\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 906us/step - loss: 0.2379 - accuracy: 0.9400\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 903us/step - loss: 0.2086 - accuracy: 0.9533\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 905us/step - loss: 0.1780 - accuracy: 0.9667\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 950us/step - loss: 0.1609 - accuracy: 0.9600\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9733\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 857us/step - loss: 0.1331 - accuracy: 0.9533\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 919us/step - loss: 0.1208 - accuracy: 0.9667\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9533\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9533\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9533\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9667\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.0927 - accuracy: 0.9667\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 916us/step - loss: 0.0830 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d3de2edf0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "model3.fit(x_values,y_values,epochs=20,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of nodes in the hidden layer seems to improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that the neural network can be trained for any number of epochs. This means that the network can keep learning from the same dataset many times. What do you think will happen if the model keeps learning from the same dataset? Do you think the network will be able to obtain very high accuracy by doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's accuracy will keep increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You were right if you think that the model's accuracy will increase as it continues to learn. You can try it on the same dataset. Run the code below and observe the accuracy. Is it higher than the accuracy from the previous model with the same setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 1s 831us/step - loss: 1.0760 - accuracy: 0.3533\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 0s 918us/step - loss: 0.8328 - accuracy: 0.5467\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 0s 910us/step - loss: 0.7014 - accuracy: 0.8533\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 0s 914us/step - loss: 0.6054 - accuracy: 0.9333\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.9467\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 0s 947us/step - loss: 0.4290 - accuracy: 0.9267\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.3665 - accuracy: 0.9333\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.9467\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.9467\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 0s 905us/step - loss: 0.2201 - accuracy: 0.9600\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 0s 868us/step - loss: 0.1790 - accuracy: 0.9600\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.1506 - accuracy: 0.9600\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.1308 - accuracy: 0.9667\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 0s 954us/step - loss: 0.1160 - accuracy: 0.9667\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9667\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9733\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 0s 903us/step - loss: 0.0925 - accuracy: 0.9667\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 0s 821us/step - loss: 0.0832 - accuracy: 0.9800\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9733\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9733\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 0s 924us/step - loss: 0.0730 - accuracy: 0.9733\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9800\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9733\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9667\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.0659 - accuracy: 0.9667\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9733\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 0s 868us/step - loss: 0.0625 - accuracy: 0.9733\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 0s 926us/step - loss: 0.0617 - accuracy: 0.9733\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9733\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9800\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 0s 860us/step - loss: 0.0572 - accuracy: 0.9800\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9733\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9800\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0573 - accuracy: 0.9733\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0569 - accuracy: 0.9800\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0531 - accuracy: 0.9733\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9800\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9733\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9800\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9733\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9733\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.0527 - accuracy: 0.9800\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0545 - accuracy: 0.9800\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9800\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9867\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.0503 - accuracy: 0.9800\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9800\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 0s 817us/step - loss: 0.0509 - accuracy: 0.9800\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9800\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9800\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9800\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 0s 932us/step - loss: 0.0503 - accuracy: 0.9800\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9800\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9800\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9800\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9867\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.0541 - accuracy: 0.9800\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0478 - accuracy: 0.9800\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 0s 924us/step - loss: 0.0472 - accuracy: 0.9867\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9667\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0477 - accuracy: 0.9800\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.0487 - accuracy: 0.9733\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9800\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - 0s 939us/step - loss: 0.0460 - accuracy: 0.9867\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - 0s 994us/step - loss: 0.0495 - accuracy: 0.9733\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.0479 - accuracy: 0.9800\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - 0s 875us/step - loss: 0.0465 - accuracy: 0.9733\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9867\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.0467 - accuracy: 0.9667\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9800\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - 0s 986us/step - loss: 0.0463 - accuracy: 0.9733\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0484 - accuracy: 0.9800\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - 0s 937us/step - loss: 0.0443 - accuracy: 0.9867\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0448 - accuracy: 0.9733\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0468 - accuracy: 0.9800\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - 0s 926us/step - loss: 0.0467 - accuracy: 0.9800\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - 0s 826us/step - loss: 0.0449 - accuracy: 0.9867\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0480 - accuracy: 0.9733\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - 0s 948us/step - loss: 0.0492 - accuracy: 0.9800\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 841us/step - loss: 0.0469 - accuracy: 0.9800\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0482 - accuracy: 0.9867\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.0462 - accuracy: 0.9800\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - 0s 926us/step - loss: 0.0473 - accuracy: 0.9733\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - 0s 814us/step - loss: 0.0532 - accuracy: 0.9733\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.0482 - accuracy: 0.9867\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - 0s 832us/step - loss: 0.0474 - accuracy: 0.9800\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - 0s 881us/step - loss: 0.0441 - accuracy: 0.9800\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - 0s 893us/step - loss: 0.0455 - accuracy: 0.9800\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - 0s 856us/step - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - 0s 990us/step - loss: 0.0450 - accuracy: 0.9800\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - 0s 891us/step - loss: 0.0461 - accuracy: 0.9800\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0527 - accuracy: 0.9667\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0470 - accuracy: 0.9800\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - 0s 948us/step - loss: 0.0468 - accuracy: 0.9867\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.0438 - accuracy: 0.9867\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0446 - accuracy: 0.9800\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - 0s 862us/step - loss: 0.0471 - accuracy: 0.9800\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9800\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0461 - accuracy: 0.9800\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0452 - accuracy: 0.9867\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - 0s 978us/step - loss: 0.0471 - accuracy: 0.9800\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.0445 - accuracy: 0.9800\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - 0s 820us/step - loss: 0.0473 - accuracy: 0.9800\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.0465 - accuracy: 0.9800\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0461 - accuracy: 0.9867\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - 0s 905us/step - loss: 0.0447 - accuracy: 0.9867\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.0456 - accuracy: 0.9800\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.0473 - accuracy: 0.9800\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.0446 - accuracy: 0.9800\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9867\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - 0s 886us/step - loss: 0.0471 - accuracy: 0.9800\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.0497 - accuracy: 0.9800\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - 0s 965us/step - loss: 0.0463 - accuracy: 0.9867\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - 0s 867us/step - loss: 0.0449 - accuracy: 0.9867\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - 0s 913us/step - loss: 0.0444 - accuracy: 0.9867\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - 0s 973us/step - loss: 0.0508 - accuracy: 0.9733\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - 0s 923us/step - loss: 0.0463 - accuracy: 0.9800\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.0449 - accuracy: 0.9867\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9867\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.0479 - accuracy: 0.9800\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0439 - accuracy: 0.9733\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0469 - accuracy: 0.9867\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.0439 - accuracy: 0.9867\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.0481 - accuracy: 0.9800\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0461 - accuracy: 0.9733\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - 0s 853us/step - loss: 0.0466 - accuracy: 0.9733\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9800\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9867\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - 0s 989us/step - loss: 0.0447 - accuracy: 0.9867\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - 0s 869us/step - loss: 0.0434 - accuracy: 0.9800\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - 0s 959us/step - loss: 0.0430 - accuracy: 0.9733\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - 0s 852us/step - loss: 0.0468 - accuracy: 0.9800\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - 0s 783us/step - loss: 0.0440 - accuracy: 0.9867\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9800\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.0444 - accuracy: 0.9867\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.0464 - accuracy: 0.9867\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - 0s 730us/step - loss: 0.0429 - accuracy: 0.9733\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - 0s 777us/step - loss: 0.0462 - accuracy: 0.9867\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - 0s 745us/step - loss: 0.0472 - accuracy: 0.9867\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - 0s 895us/step - loss: 0.0468 - accuracy: 0.9800\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - 0s 950us/step - loss: 0.0465 - accuracy: 0.9800\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.0479 - accuracy: 0.9800\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - 0s 947us/step - loss: 0.0451 - accuracy: 0.9800\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0419 - accuracy: 0.9867\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9800\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9867\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - 0s 776us/step - loss: 0.0476 - accuracy: 0.9867\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - 0s 934us/step - loss: 0.0455 - accuracy: 0.9867\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - 0s 756us/step - loss: 0.0431 - accuracy: 0.9733\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - 0s 935us/step - loss: 0.0434 - accuracy: 0.9867\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9800\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0431 - accuracy: 0.9867\n",
      "Epoch 160/200\n",
      "150/150 [==============================] - 0s 817us/step - loss: 0.0447 - accuracy: 0.9867\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0460 - accuracy: 0.9867\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - 0s 790us/step - loss: 0.0432 - accuracy: 0.9867\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - 0s 768us/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.0458 - accuracy: 0.9867\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - 0s 763us/step - loss: 0.0427 - accuracy: 0.9867\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - 0s 727us/step - loss: 0.0481 - accuracy: 0.9733\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9800\n",
      "Epoch 168/200\n",
      "150/150 [==============================] - 0s 767us/step - loss: 0.0431 - accuracy: 0.9867\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - 0s 666us/step - loss: 0.0474 - accuracy: 0.9800\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - 0s 676us/step - loss: 0.0472 - accuracy: 0.9867\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - 0s 820us/step - loss: 0.0436 - accuracy: 0.9867\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - 0s 856us/step - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - 0s 858us/step - loss: 0.0475 - accuracy: 0.9867\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9800\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9800\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0438 - accuracy: 0.9867\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - 0s 669us/step - loss: 0.0466 - accuracy: 0.9800\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - 0s 796us/step - loss: 0.0443 - accuracy: 0.9800\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0459 - accuracy: 0.9800\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.0434 - accuracy: 0.9867\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - 0s 907us/step - loss: 0.0437 - accuracy: 0.9800\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.9867\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9733\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9800\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0437 - accuracy: 0.9867\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - 0s 879us/step - loss: 0.0442 - accuracy: 0.9867\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - 0s 966us/step - loss: 0.0449 - accuracy: 0.9867\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9800\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.0462 - accuracy: 0.9800\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - 0s 965us/step - loss: 0.0420 - accuracy: 0.9867\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9867\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.0412 - accuracy: 0.9867\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - 0s 961us/step - loss: 0.0439 - accuracy: 0.9867\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - 0s 820us/step - loss: 0.0440 - accuracy: 0.9867\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0421 - accuracy: 0.9867\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - 0s 846us/step - loss: 0.0430 - accuracy: 0.9867\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - 0s 848us/step - loss: 0.0439 - accuracy: 0.9867\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.0445 - accuracy: 0.9800\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - 0s 898us/step - loss: 0.0436 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d3f1fabb0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model4 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or input_layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model4.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model4.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Train model with x_values and y_values. Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "model4.fit(x_values,y_values,epochs=200,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe from the printout that the accuracy score is now above 90%. Wow! It seems amazing that just increasing the number of epochs will allow the accuracy to increase. Do you think it is good that the model is so highly accuracte on the data that it has trained on? Just imagine, will a soccer player do well in a match if he/she only trains extremely hard on scoring a goal from a specific spot on the field? Or will a baker be able to bake a delicious cake based on a customer's request if he/she only learns how to bake a cake that is a specific flavour, shape and size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soccer player will not perform well in a match as he/she may not be able to shoot accurately from other parts of the field. The baker will not be able to bake a delicious cake as he/she will only know 1 specific flavour.\n",
    "\n",
    "The idea behind this question is the concept of overfitting. If the model overfits, it will not be able to generalise to properly predict data that it has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the concept of overfitting and also applies to all machine learning techniques. Once the technique has fitted too acurrately on the dataset, the trained technique will not be able to generalise to other data that it has not seen before. As such, we usually only train the technique on a fraction of the dataset that we have and keep the remainder as a test or validation set to see if the model has overfitted. When training on the training set, the accuracy of the model on the test set should increase. However, at the point of overfitting, the accuracy on the test set will start to decrease. If you see that the test accuracy has begin to increase after a certain epoch, you should not train the model any further.\n",
    "\n",
    "Let us apply the split between a train and a test set to the dataset that we are using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to decide how much data we want to keep and prevent the model from training on. Usually, we withold about 20% to 30% of the dataset. In this example, we will keep 25 percent of the dataset as the test/validation set. We can use the train_test_split function from the sklearn library. Try the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train: 112\n",
      "Number of rows in x_test: 38\n",
      "Number of rows in y_train: 112\n",
      "Number of rows in y_test: 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract out original x_values from the dataframe df. \n",
    "# We have to re-extract the x_values as the standardisation should only be based on the data that the model will train with.\n",
    "# Thus, we have to split the data first before standardising.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.25 indicates that 25% of the datapoints will be in x_test and y_test whereas 75% will be in x_train and y_train.\n",
    "# random_state=10 is used to ensure that the split is the same everytime you run the code below. \n",
    "# This is because the split is done randomly everytime. The same random_state is the only way to ensure the same split everytime.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.25,random_state=10)\n",
    "\n",
    "# Check the number of rows in x_train, x_test, y_train and y_test\n",
    "print(\"Number of rows in x_train:\", x_train.shape[0])\n",
    "print(\"Number of rows in x_test:\", x_test.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# We can now standardise the x values.\n",
    "# Initialise the StandardScaler\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# Standardise the x_train values using .fit_transform\n",
    "x_train = standardise.fit_transform(x_train)\n",
    "\n",
    "# Standardise the x_test values using .transform. \n",
    "# There is no need to fit the data as the standardisation should be the same as that of x_train.\n",
    "x_test = standardise.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own code to split the dataset into train and test/validation sets by witholding 20% of the dataset as test/validation set. Use x_train2, x_test2, y_train2 and y_test2 as your variables. Check that the number of rows are correct for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train2: 120\n",
      "Number of rows in x_test2: 30\n",
      "Number of rows in y_train2: 120\n",
      "Number of rows in y_test2: 30\n"
     ]
    }
   ],
   "source": [
    "# Extract out original x_values from the dataframe df. \n",
    "# We have to re-extract the x_values as the standardisation should only be based on the data that the model will train with.\n",
    "# Thus, we have to split the data first before standardising.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.2 indicates that 20% of the datapoints will be in x_test and y_test whereas 80% will be in x_train and y_train.\n",
    "# random_state=10 is used to ensure that the split is the same everytime you run the code below. \n",
    "# This is because the split is done randomly everytime. The same random_state is the only way to ensure the same split everytime.\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_values,y_values,test_size=0.2,random_state=10)\n",
    "\n",
    "# Check the number of rows in x_train2, x_test2, y_train2 and y_test2\n",
    "print(\"Number of rows in x_train2:\", x_train2.shape[0])\n",
    "print(\"Number of rows in x_test2:\", x_test2.shape[0])\n",
    "print(\"Number of rows in y_train2:\", y_train2.shape[0])\n",
    "print(\"Number of rows in y_test2:\", y_test2.shape[0])\n",
    "\n",
    "# We can now standardise the x values.\n",
    "# Initialise the StandardScaler\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# Standardise the x_train values using .fit_transform\n",
    "x_train2 = standardise.fit_transform(x_train2)\n",
    "\n",
    "# Standardise the x_test values using .transform. \n",
    "# There is no need to fit the data as the standardisation should be the same as that of x_train.\n",
    "x_test2 = standardise.transform(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use x_train2, x_test2, y_train2 and y_test2 to train a neural network. Write a code to create a neural network that has 2 hidden layers with 6 nodes each and 1 output layer with 3 nodes. The activation for the input hidden layer is 'relu' wheras the activation for the output layer is 'softmax'. The opitmizer to use is 'adam' and the loss should be 'categorical_crossentropy'. The metrics should be 'accuracy'. Use model_val as the model variable. Print the model summary after you compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model_val = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model_val.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model_val.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model_val.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model_val.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "print(model_val.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train model_val using x_train and y_train. We will also test the accuracy after each epoch using x_test and y_test. Try the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "112/112 [==============================] - 1s 3ms/step - loss: 0.8860 - accuracy: 0.6875 - val_loss: 0.8127 - val_accuracy: 0.6579\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.7066 - accuracy: 0.7321 - val_loss: 0.6646 - val_accuracy: 0.7105\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.7500 - val_loss: 0.5731 - val_accuracy: 0.7105\n",
      "Epoch 4/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.4924 - accuracy: 0.7857 - val_loss: 0.5196 - val_accuracy: 0.7105\n",
      "Epoch 5/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.8036 - val_loss: 0.4717 - val_accuracy: 0.7105\n",
      "Epoch 6/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.4081 - accuracy: 0.8214 - val_loss: 0.4332 - val_accuracy: 0.7632\n",
      "Epoch 7/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.3770 - accuracy: 0.8571 - val_loss: 0.3999 - val_accuracy: 0.7632\n",
      "Epoch 8/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8571 - val_loss: 0.3689 - val_accuracy: 0.8421\n",
      "Epoch 9/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8661 - val_loss: 0.3373 - val_accuracy: 0.8947\n",
      "Epoch 10/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.3002 - accuracy: 0.8661 - val_loss: 0.3134 - val_accuracy: 0.8947\n",
      "Epoch 11/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.8929 - val_loss: 0.2870 - val_accuracy: 0.9211\n",
      "Epoch 12/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8929 - val_loss: 0.2627 - val_accuracy: 0.9211\n",
      "Epoch 13/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.9196 - val_loss: 0.2421 - val_accuracy: 0.9211\n",
      "Epoch 14/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9107 - val_loss: 0.2243 - val_accuracy: 0.9737\n",
      "Epoch 15/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9375 - val_loss: 0.2120 - val_accuracy: 0.9737\n",
      "Epoch 16/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9107 - val_loss: 0.1977 - val_accuracy: 0.9737\n",
      "Epoch 17/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9554 - val_loss: 0.1866 - val_accuracy: 0.9737\n",
      "Epoch 18/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9554 - val_loss: 0.1733 - val_accuracy: 0.9737\n",
      "Epoch 19/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9464 - val_loss: 0.1611 - val_accuracy: 0.9737\n",
      "Epoch 20/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9554 - val_loss: 0.1550 - val_accuracy: 0.9737\n",
      "Epoch 21/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9554 - val_loss: 0.1454 - val_accuracy: 0.9737\n",
      "Epoch 22/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.9464 - val_loss: 0.1352 - val_accuracy: 0.9737\n",
      "Epoch 23/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1499 - accuracy: 0.9554 - val_loss: 0.1284 - val_accuracy: 0.9737\n",
      "Epoch 24/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9464 - val_loss: 0.1257 - val_accuracy: 0.9737\n",
      "Epoch 25/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9554 - val_loss: 0.1194 - val_accuracy: 0.9737\n",
      "Epoch 26/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9464 - val_loss: 0.1118 - val_accuracy: 0.9737\n",
      "Epoch 27/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9554 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9464 - val_loss: 0.1054 - val_accuracy: 0.9737\n",
      "Epoch 29/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9554 - val_loss: 0.0979 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.9464 - val_loss: 0.0943 - val_accuracy: 0.9737\n",
      "Epoch 31/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.9643 - val_loss: 0.0913 - val_accuracy: 0.9737\n",
      "Epoch 32/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9464 - val_loss: 0.0898 - val_accuracy: 0.9737\n",
      "Epoch 33/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9375 - val_loss: 0.0892 - val_accuracy: 0.9737\n",
      "Epoch 34/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9464 - val_loss: 0.0849 - val_accuracy: 0.9737\n",
      "Epoch 35/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9554 - val_loss: 0.0817 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.1016 - accuracy: 0.9554 - val_loss: 0.0784 - val_accuracy: 0.9737\n",
      "Epoch 37/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9554 - val_loss: 0.0767 - val_accuracy: 0.9737\n",
      "Epoch 38/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9464 - val_loss: 0.0770 - val_accuracy: 0.9737\n",
      "Epoch 39/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9554 - val_loss: 0.0745 - val_accuracy: 0.9737\n",
      "Epoch 40/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9554 - val_loss: 0.0717 - val_accuracy: 0.9737\n",
      "Epoch 41/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0930 - accuracy: 0.9554 - val_loss: 0.0715 - val_accuracy: 0.9737\n",
      "Epoch 42/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9643 - val_loss: 0.0673 - val_accuracy: 0.9737\n",
      "Epoch 43/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9643 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9643 - val_loss: 0.0668 - val_accuracy: 0.9737\n",
      "Epoch 45/50\n",
      "112/112 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9554 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9554 - val_loss: 0.0615 - val_accuracy: 0.9737\n",
      "Epoch 47/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9643 - val_loss: 0.0619 - val_accuracy: 0.9737\n",
      "Epoch 48/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9554 - val_loss: 0.0600 - val_accuracy: 0.9737\n",
      "Epoch 49/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9643 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "112/112 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9643 - val_loss: 0.0581 - val_accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d405b3b20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "# Validation_data will allow us to input in the test/validation datasets. This will allow us to see the model accuracy on the test/validation set.\n",
    "model_val.fit(x_train,y_train,epochs=50,shuffle=True,validation_data=(x_test,y_test), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the printout now shows the validation accuracy as well? Is the validation accuracy higher or lower than the training accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is lower. Usually, the validation accuracy will be lower than the training accuracy as the data in the test/validation set is not used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the model to identify flower types for newly gathered data. For example, imagine that your friend has measured the sepal_length, sepal_width, petal_length and petal_width of some flowers and saved the data in a file called \"iris_predict.data\". Your friend wants to find the flower types for these flowers based on the values measured. Would you be able to use your model to help your friend? What are the flower types that your friend measured? Try the codes below!\n",
    "<font color=blue>Hint: You can use model.predict method to obtain the flower types for your friend. Additionally, the values returned by the .predict method will be the probability of each flower type that the model thinks should be assigned to each data row. As such, the higher the probablity, the more confident the model is that the flower type predicted is correct. For example, if the predicted values are very high in the second column, then the model thinks that the flower type is versicolor. You will also need to scale your data before obtaining the flower types. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"./[Dataset] Module 17 (iris_predict).data\",header=None)\n",
    "names = [\"sepal_length\", \"sepal_width\",\"petal_length\", \"petal_width\"]\n",
    "df2.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = df2[['sepal_length','sepal_width','petal_length','petal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_scale = standardise.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = model_val.predict(x_new_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the flower types for each data row? You can use the [argmax](https://www.geeksforgeeks.org/numpy-argmax-python/) function to help you identify the flower types from y_new. Try the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_types = []\n",
    "for ii in range(0,y_new.shape[0]):\n",
    "    flower_types.append(np.argmax(y_new[ii,:]))\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to match the flower type numbers back to the different types (Setosa, Versicolor, Virginica)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flower types are Setosa, Setosa, Setosa, Virginica, Virginica, Virginica, Virginica, Virginica, Virginica and Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have successfully learnt how to create an artificial neural network model and trained it. Artficial neural networks are extremely powerful tools that can be used on very large datasets. For example, if you have a couple of hundred columns/features and over 100000 data points, it may be beneficial to use artificial neural networks instead of other machine learning techniques. Just remember, there are no strict rules on the number of nodes in the hidden layer or the number of hidden layers. Experiment with different neural networks to see which one provides the best result for your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
